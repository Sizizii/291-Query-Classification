{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from .json file\n",
    "\n",
    "with open(\"plain_statement_5000.json\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "plain_sql = [item['sql'] for item in json_data]\n",
    "plain_sql = [sql.lower() for sql in plain_sql]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into tokens\n",
    "\n",
    "pattern = r'[\\s()\\-,:;]'\n",
    "string_literal_pattern = r\"'([^']*)'\"\n",
    "placeholder = \"<string>\"\n",
    "\n",
    "# replace content inside single quotes by <string>\n",
    "plain_sql_ph = [re.sub(string_literal_pattern, placeholder, sql) for sql in plain_sql]\n",
    "\n",
    "# split the statements with placeholder\n",
    "tokenized_sql = [re.split(pattern, sql) for sql in plain_sql_ph]\n",
    "\n",
    "# remove empty tokens\n",
    "tokenized_sql = [token for token in tokenized_sql if token]\n",
    "\n",
    "# replace numbers by placeholder\n",
    "for sql in tokenized_sql:\n",
    "    for i, token in enumerate(sql):\n",
    "        # if re.match(r'^[\\'\\\"].*[\\'\\\"]$', token):  # Check if token is a string literal\n",
    "        #     sql[i] = '<string>'\n",
    "        if re.match(r'^[0-9]+(\\.[0-9]+)?$', token):  # Check if token is a number\n",
    "            sql[i] = '<number>'\n",
    "\n",
    "# remove empty tokens\n",
    "for i, sql in enumerate(tokenized_sql):\n",
    "    tokenized_sql[i] = [token for token in tokenized_sql[i] if token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocab\n",
    "vocab_set = set()\n",
    "for sql in tokenized_sql:\n",
    "    vocab_set.update(sql)\n",
    "\n",
    "vocab = {word: idx for idx, word in enumerate(vocab_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\"lineitem\".\"l_tax\"': 0, '\"nation\".\"n_name\"': 1, '\"supplier\".\"s_nationkey\"': 2, '<=': 3, 'count': 4, '\"customer\".\"c_mktsegment\"': 5, '\"nation\".\"n_comment\"': 6, '\"orders\".\"o_orderpriority\"': 7, '\"orders\"': 8, '\"orders\".\"o_orderdate\"': 9, '\"part\".\"p_container\"': 10, '\"customer\".\"c_nationkey\"': 11, '\"partsupp\".\"ps_supplycost\"': 12, '\"lineitem\".\"l_orderkey\"': 13, '+': 14, '\"orders\".\"o_totalprice\"': 15, '*': 16, '\"lineitem\".\"l_commitdate\"': 17, '\"region\".\"r_regionkey\"': 18, 'on': 19, '\"lineitem\".\"l_receiptdate\"': 20, 'sum': 21, '\"supplier\".\"s_comment\"': 22, '>=': 23, '\"supplier\"': 24, '\"lineitem\".\"l_partkey\"': 25, '\"supplier\".\"s_phone\"': 26, '\"partsupp\".\"ps_suppkey\"': 27, '\"part\"': 28, '\"orders\".\"o_clerk\"': 29, '\"lineitem\"': 30, '\"orders\".\"o_shippriority\"': 31, '\"customer\".\"c_custkey\"': 32, '\"customer\"': 33, '\"nation\"': 34, '=': 35, '\"supplier\".\"s_suppkey\"': 36, '\"lineitem\".\"l_shipinstruct\"': 37, '!=': 38, 'from': 39, 'avg': 40, '\"partsupp\".\"ps_availqty\"': 41, '\"lineitem\".\"l_linestatus\"': 42, '\"region\".\"r_comment\"': 43, '\"lineitem\".\"l_returnflag\"': 44, '\"part\".\"p_retailprice\"': 45, '<string>': 46, '\"lineitem\".\"l_shipmode\"': 47, '\"lineitem\".\"l_linenumber\"': 48, '\"part\".\"p_mfgr\"': 49, '\"supplier\".\"s_address\"': 50, '\"supplier\".\"s_acctbal\"': 51, '\"region\"': 52, '\"part\".\"p_type\"': 53, 'where': 54, '\"orders\".\"o_orderstatus\"': 55, '\"customer\".\"c_acctbal\"': 56, '\"lineitem\".\"l_shipdate\"': 57, '\"lineitem\".\"l_extendedprice\"': 58, '\"lineitem\".\"l_suppkey\"': 59, '\"orders\".\"o_orderkey\"': 60, '\"partsupp\"': 61, '\"partsupp\".\"ps_partkey\"': 62, '\"part\".\"p_brand\"': 63, '<number>': 64, '\"nation\".\"n_nationkey\"': 65, 'join': 66, '\"nation\".\"n_regionkey\"': 67, '\"lineitem\".\"l_quantity\"': 68, '\"part\".\"p_size\"': 69, '\"supplier\".\"s_name\"': 70, 'and': 71, '\"lineitem\".\"l_discount\"': 72, 'select': 73, '\"part\".\"p_partkey\"': 74, '\"region\".\"r_name\"': 75, '\"orders\".\"o_custkey\"': 76}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tokens to indices for each sample\n",
    "indices = [[vocab[token] for token in sql] for sql in tokenized_sql]\n",
    "\n",
    "# define embedding layer\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 10\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# convert indices to PyTorch tensors\n",
    "indices = [torch.LongTensor(index) for index in indices]\n",
    "embedded_X = [embedding(index) for index in indices]\n",
    "print(embedded_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labels\n",
    "\n",
    "label = [item['runtime_ms'] for item in json_data]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
