{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(path):\n",
    "  with open(path) as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "  plain_sql = [item['sql'] for item in json_data]\n",
    "  plain_sql = [sql.lower() for sql in plain_sql]\n",
    "\n",
    "  # split data into tokens\n",
    "\n",
    "  pattern = r'[\\s()\\-,:;]'\n",
    "  string_literal_pattern = r\"'([^']*)'\"\n",
    "  placeholder = \"<string>\"\n",
    "  \n",
    "  # replace content inside single quotes by <string>\n",
    "  plain_sql_ph = [re.sub(string_literal_pattern, placeholder, sql) for sql in plain_sql]\n",
    "  \n",
    "  # split the statements with placeholder\n",
    "  tokenized_sql = [re.split(pattern, sql) for sql in plain_sql_ph]\n",
    "  \n",
    "  # remove empty tokens\n",
    "  tokenized_sql = [token for token in tokenized_sql if token]\n",
    "  \n",
    "  # replace numbers by placeholder\n",
    "  # for sql in tokenized_sql:\n",
    "  #     for i, token in enumerate(sql):\n",
    "  #         # if re.match(r'^[\\'\\\"].*[\\'\\\"]$', token):  # Check if token is a   string literal\n",
    "  #         #     sql[i] = '<string>'\n",
    "  #         if re.match(r'^[0-9]+(\\.[0-9]+)?$', token):  # Check if token is a  number\n",
    "  #             sql[i] = '<number>'\n",
    "  \n",
    "  # remove empty tokens\n",
    "  for i, sql in enumerate(tokenized_sql):\n",
    "      tokenized_sql[i] = [token for token in tokenized_sql[i] if token]\n",
    "\n",
    "    # build the vocab\n",
    "  vocab_set = set()\n",
    "  for sql in tokenized_sql:\n",
    "      vocab_set.update(sql)\n",
    "\n",
    "  vocab_dict = {word: idx for idx, word in enumerate(vocab_set)}\n",
    "\n",
    "  # get the runtimes\n",
    "  runtime = [item['runtime_ms'] for item in json_data]\n",
    "  runtime = np.array(runtime)\n",
    "\n",
    "  # classify the runtimes, label 0 for runtime <=3000ms, 1 for runtime >3000ms\n",
    "  label = np.where(runtime > 3000, 1, 0)\n",
    "\n",
    "  res = [\" \".join(tokenized).lower() for tokenized in tokenized_sql]\n",
    "\n",
    "  return vocab_set, res, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict_w2v = {}\n",
    "with open(\"glove.6B.300d.txt\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embedding_dict_w2v[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "# Path to the GloVe file (after unzipping)\n",
    "glove_file = 'glove.6B.300d.txt'  # Modify the path and file name as necessary\n",
    "\n",
    "# Function to load GloVe vectors into a Gensim KeyedVectors object\n",
    "def load_glove_model(glove_file):\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        glove_vectors = {}\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            glove_vectors[word] = vector\n",
    "    \n",
    "    # Create a KeyedVectors object\n",
    "    word2vec_model = gensim.models.KeyedVectors(vector_size=len(vector))\n",
    "    word2vec_model.add_vectors(list(glove_vectors.keys()), list(glove_vectors.values()))\n",
    "    return word2vec_model\n",
    "\n",
    "# Load the GloVe model\n",
    "word2vec_model = load_glove_model(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(sentence, embeddings, dimension=50):\n",
    "    words = sentence.lower().split()\n",
    "    word_vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "    \n",
    "    if not word_vectors:\n",
    "        # Return a zero vector if no words are found in the embeddings\n",
    "        return np.zeros(dimension)\n",
    "    \n",
    "    # Combine the word vectors by averaging\n",
    "    sentence_vector = np.mean(word_vectors, axis=0)\n",
    "    return sentence_vector\n",
    "\n",
    "def vectorize_w2v(plain_sql, embeddings, dimension=50):\n",
    "    vectors = [sentence_to_vector(sentence, embeddings, dimension) for sentence in plain_sql]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set_15k, plain_sql_ph_15k, label_15k = tokenize(\"../../datasets/plain_text/plain_statement.json\")\n",
    "vocab_set_5k, plain_sql_ph_5k, label_5k = tokenize(\"../../datasets/plain_text/plain_statement_5000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer_15k = TfidfVectorizer()\n",
    "tfidf_matrix_15k = tfidf_vectorizer_15k.fit_transform(plain_sql_ph_15k)\n",
    "feature_names_15k = tfidf_vectorizer_15k.get_feature_names_out()\n",
    "\n",
    "tfidf_vectorizer_5k = TfidfVectorizer()\n",
    "tfidf_matrix_5k = tfidf_vectorizer_5k.fit_transform(plain_sql_ph_5k)\n",
    "feature_names_5k = tfidf_vectorizer_5k.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_weighted_word2vec(tfidf_matrix, feature_names, word2vec_model):\n",
    "    doc_vectors = []\n",
    "    for doc_idx in range(tfidf_matrix.shape[0]):\n",
    "        feature_index = tfidf_matrix[doc_idx,:].nonzero()[1]\n",
    "        tfidf_scores = zip(feature_index, [tfidf_matrix[doc_idx, x] for x in feature_index])\n",
    "        \n",
    "        weighted_word_vec = np.zeros((word2vec_model.vector_size,))\n",
    "        weight_sum = 0\n",
    "        for word_idx, score in tfidf_scores:\n",
    "            word = feature_names[word_idx]\n",
    "            if word in word2vec_model.key_to_index:\n",
    "                weighted_word_vec += score * word2vec_model[word]\n",
    "                weight_sum += score\n",
    "        \n",
    "        if weight_sum != 0:\n",
    "            weighted_word_vec /= weight_sum\n",
    "        \n",
    "        doc_vectors.append(weighted_word_vec)\n",
    "    return np.array(doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_w2v_mat_5k = get_weighted_word2vec(tfidf_matrix_5k, feature_names_5k, word2vec_model)\n",
    "weighted_w2v_mat_15k = get_weighted_word2vec(tfidf_matrix_15k, feature_names_15k, word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_mat_15k = vectorize_w2v(plain_sql_ph_15k, embedding_dict_w2v, dimension=300)\n",
    "w2v_mat_5k = vectorize_w2v(plain_sql_ph_5k, embedding_dict_w2v, dimension=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=vocab_set_5k)\n",
    "cv_mat_15k = vectorizer.fit_transform(plain_sql_ph_15k)\n",
    "cv_mat_5k = vectorizer.fit_transform(plain_sql_ph_5k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_mat_15k = tfidf_vectorizer.fit_transform(plain_sql_ph_15k)\n",
    "tfidf_mat_15k = tfidf_mat_15k.toarray()\n",
    "\n",
    "tfidf_mat_5k = tfidf_vectorizer.fit_transform(plain_sql_ph_5k)\n",
    "tfidf_mat_5k = tfidf_mat_5k.toarray()\n",
    "# tfidf_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(cv_mat, tfidf_mat, label):\n",
    "  X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_mat, label, test_size=0.2, random_state=seed)\n",
    "\n",
    "  X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(cv_mat, label, test_size=0.2, random_state=seed)\n",
    "\n",
    "  \n",
    "  def experiment_cv(model):\n",
    "    model.fit(X_train_cv, y_train_cv)\n",
    "    y_pred = model.predict(X_test_cv)\n",
    "\n",
    "    accuracy = accuracy_score(y_test_cv, y_pred)\n",
    "\n",
    "    precision_positive = precision_score(y_test_cv, y_pred, pos_label=1)\n",
    "    recall_positive = recall_score(y_test_cv, y_pred, pos_label=1)\n",
    "    precision_negative = precision_score(y_test_cv, y_pred, pos_label=0)\n",
    "    recall_negative = recall_score(y_test_cv, y_pred, pos_label=0)\n",
    "\n",
    "    return accuracy, precision_positive, recall_positive, precision_negative, recall_negative\n",
    "\n",
    "  def experiment_tfidf(model):\n",
    "    model.fit(X_train_tfidf, y_train_tfidf)\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "    accuracy = accuracy_score(y_test_tfidf, y_pred)\n",
    "\n",
    "    precision_positive = precision_score(y_test_tfidf, y_pred, pos_label=1)\n",
    "    recall_positive = recall_score(y_test_tfidf, y_pred, pos_label=1)\n",
    "    precision_negative = precision_score(y_test_tfidf, y_pred, pos_label=0)\n",
    "    recall_negative = recall_score(y_test_tfidf, y_pred, pos_label=0)\n",
    "\n",
    "    return accuracy, precision_positive, recall_positive, precision_negative, recall_negative\n",
    "  \n",
    "  models = {\"LR\": LogisticRegression(max_iter=1000), \"XGB\": xgb.XGBClassifier(), \"RF\": RandomForestClassifier(n_estimators=100, random_state=seed)}\n",
    "\n",
    "  result_cv = dict()\n",
    "  result_tfidf = dict()\n",
    "\n",
    "  for key in models:\n",
    "    model = models[key]\n",
    "\n",
    "    result_cv[key] = experiment_cv(model)\n",
    "    print(f\"Completed training of model {key} for BoW\")\n",
    "    result_tfidf[key] = experiment_tfidf(model)\n",
    "    print(f\"Completed training of model {key} for TF-IDF\")\n",
    "\n",
    "  result_cv[\"SVM\"] = experiment_cv(SVC())\n",
    "  print(\"Completed training of model SVM for BoW\")\n",
    "\n",
    "  return result_cv, result_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp_w2v(w2v_mat, label):\n",
    "  X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(w2v_mat, label, test_size=0.2, random_state=seed)\n",
    "\n",
    "  def experiment_w2v(model):\n",
    "    model.fit(X_train_w2v, y_train_w2v)\n",
    "    y_pred = model.predict(X_test_w2v)\n",
    "\n",
    "    accuracy = accuracy_score(y_test_w2v, y_pred)\n",
    "\n",
    "    precision_positive = precision_score(y_test_w2v, y_pred, pos_label=1)\n",
    "    recall_positive = recall_score(y_test_w2v, y_pred, pos_label=1)\n",
    "    precision_negative = precision_score(y_test_w2v, y_pred, pos_label=0)\n",
    "    recall_negative = recall_score(y_test_w2v, y_pred, pos_label=0)\n",
    "\n",
    "    return accuracy, precision_positive, recall_positive, precision_negative, recall_negative\n",
    "  \n",
    "  models = {\"LR\": LogisticRegression(max_iter=1000), \"XGB\": xgb.XGBClassifier(), \"RF\": RandomForestClassifier(n_estimators=100, random_state=seed)}\n",
    "\n",
    "  result_w2v = dict()\n",
    "\n",
    "  for key in models:\n",
    "    model = models[key]\n",
    "    result_w2v[key] = experiment_w2v(model)\n",
    "    print(f\"Completed training of model {key} for Word2Vector\")\n",
    "  \n",
    "  result_w2v[\"SVM\"] = experiment_w2v(SVC())\n",
    "  print(\"Completed training of model SVM for BoW\")\n",
    "\n",
    "  return result_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed training of model LR for Word2Vector\n",
      "Completed training of model XGB for Word2Vector\n",
      "Completed training of model RF for Word2Vector\n",
      "Completed training of model SVM for BoW\n",
      "Completed training of model LR for Word2Vector\n",
      "Completed training of model XGB for Word2Vector\n",
      "Completed training of model RF for Word2Vector\n",
      "Completed training of model SVM for BoW\n",
      "Results of tfidf w2v - 5k\n",
      "LR: (0.792, 0.691358024691358, 0.6746987951807228, 0.8402366863905325, 0.8502994011976048)\n",
      "XGB: (0.767, 0.6542056074766355, 0.6325301204819277, 0.8203240058910162, 0.8338323353293413)\n",
      "RF: (0.776, 0.6719745222929936, 0.6355421686746988, 0.8236151603498543, 0.8458083832335329)\n",
      "SVM: (0.78, 0.6761006289308176, 0.6475903614457831, 0.8284457478005866, 0.8458083832335329)\n",
      "Results of tfidf w2v - 15k\n",
      "LR: (0.7927970065481759, 0.6993464052287581, 0.6229985443959243, 0.8302752293577982, 0.8731909028256375)\n",
      "XGB: (0.7675397567820393, 0.6512738853503185, 0.5953420669577875, 0.8158940397350993, 0.8490696071674707)\n",
      "RF: (0.7712815715622077, 0.671875, 0.5633187772925764, 0.8079385403329066, 0.8697450034458993)\n",
      "SVM: (0.7890551917680075, 0.6884984025559105, 0.6273653566229985, 0.8306878306878307, 0.8656099241902137)\n"
     ]
    }
   ],
   "source": [
    "r_tfidf_w2v_5k = run_exp_w2v(weighted_w2v_mat_5k, label_5k)\n",
    "r_tfidf_w2v_15k = run_exp_w2v(weighted_w2v_mat_15k, label_15k)\n",
    "\n",
    "print(\"Results of tfidf w2v - 5k\")\n",
    "for key, value in r_tfidf_w2v_5k.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"Results of tfidf w2v - 15k\")\n",
    "for key, value in r_tfidf_w2v_15k.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed training of model LR for Word2Vector\n",
      "Completed training of model XGB for Word2Vector\n",
      "Completed training of model RF for Word2Vector\n",
      "Completed training of model SVM for BoW\n",
      "Completed training of model LR for Word2Vector\n",
      "Completed training of model XGB for Word2Vector\n",
      "Completed training of model RF for Word2Vector\n",
      "Completed training of model SVM for BoW\n",
      "Results of w2v - 5k\n",
      "LR: (0.724, 0.5921052631578947, 0.5421686746987951, 0.7816091954022989, 0.8143712574850299)\n",
      "XGB: (0.73, 0.6, 0.5602409638554217, 0.7884057971014493, 0.8143712574850299)\n",
      "RF: (0.736, 0.6180555555555556, 0.536144578313253, 0.7837078651685393, 0.8353293413173652)\n",
      "SVM: (0.725, 0.6, 0.5150602409638554, 0.7748251748251749, 0.8293413173652695)\n",
      "Results of w2v - 15k\n",
      "LR: (0.7188961646398503, 0.5820610687022901, 0.44395924308588064, 0.7633209417596035, 0.8490696071674707)\n",
      "XGB: (0.7572497661365762, 0.6329113924050633, 0.5822416302765647, 0.8094289508632138, 0.8401102687801516)\n",
      "RF: (0.7525724976613658, 0.6338983050847458, 0.5443959243085881, 0.7978036175710594, 0.8511371467953136)\n",
      "SVM: (0.725912067352666, 0.6339522546419099, 0.34788937409024745, 0.7455990914253265, 0.9048931771192281)\n"
     ]
    }
   ],
   "source": [
    "r_w2v_5k = run_exp_w2v(w2v_mat_5k, label_5k)\n",
    "r_w2v_15k = run_exp_w2v(w2v_mat_15k, label_15k)\n",
    "\n",
    "print(\"Results of w2v - 5k\")\n",
    "for key, value in r_w2v_5k.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"Results of w2v - 15k\")\n",
    "for key, value in r_w2v_15k.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed training of model LR for BoW\n",
      "Completed training of model LR for TF-IDF\n",
      "Completed training of model XGB for BoW\n",
      "Completed training of model XGB for TF-IDF\n",
      "Completed training of model RF for BoW\n",
      "Completed training of model RF for TF-IDF\n",
      "Completed training of model SVM for BoW\n",
      "Completed training of model LR for BoW\n",
      "Completed training of model LR for TF-IDF\n",
      "Completed training of model XGB for BoW\n",
      "Completed training of model XGB for TF-IDF\n",
      "Completed training of model RF for BoW\n",
      "Completed training of model RF for TF-IDF\n",
      "Completed training of model SVM for BoW\n"
     ]
    }
   ],
   "source": [
    "r_cv_5k, r_tfidf_5k = run_exp(cv_mat_5k, tfidf_mat_5k, label_5k)\n",
    "r_cv_15k, r_tfidf_15k = run_exp(cv_mat_15k, tfidf_mat_15k, label_15k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of BoW - 5k\n",
      "LR: (0.719, 0.5825242718446602, 0.5421686746987951, 0.7800289435600579, 0.8068862275449101)\n",
      "XGB: (0.727, 0.6013745704467354, 0.5271084337349398, 0.7785613540197461, 0.8263473053892215)\n",
      "RF: (0.683, 0.5252525252525253, 0.46987951807228917, 0.7496443812233285, 0.7889221556886228)\n",
      "SVM: (0.739, 0.6651162790697674, 0.4307228915662651, 0.759235668789809, 0.8922155688622755)\n",
      "Results of BoW - 15k\n",
      "LR: (0.7179607109448082, 0.584, 0.42503639010189226, 0.7588522588522588, 0.8566505858028945)\n",
      "XGB: (0.7277829747427502, 0.6035502958579881, 0.44541484716157204, 0.7664009809932557, 0.8614748449345279)\n",
      "RF: (0.7142188961646398, 0.5688405797101449, 0.4570596797671033, 0.7648171500630517, 0.8359751895244659)\n",
      "SVM: (0.7202993451824135, 0.6377708978328174, 0.29985443959243085, 0.7349862258953168, 0.9193659545141282)\n"
     ]
    }
   ],
   "source": [
    "print(\"Results of BoW - 5k\")\n",
    "for key, value in r_cv_5k.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"Results of BoW - 15k\")\n",
    "for key, value in r_cv_15k.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of TF-IDF - 5k\n",
      "LR: (0.795, 0.6906906906906907, 0.6927710843373494, 0.8470764617691154, 0.8458083832335329)\n",
      "XGB: (0.786, 0.6766467065868264, 0.6807228915662651, 0.8408408408408409, 0.8383233532934131)\n",
      "RF: (0.795, 0.6895522388059702, 0.6957831325301205, 0.8481203007518797, 0.844311377245509)\n",
      "Results of TF-IDF - 15k\n",
      "LR: (0.8021515434985969, 0.7163934426229508, 0.636098981077147, 0.8363874345549738, 0.8807718814610613)\n",
      "XGB: (0.7792329279700655, 0.6616541353383458, 0.6404657933042213, 0.8323150033944331, 0.844934527911785)\n",
      "RF: (0.7820392890551918, 0.678513731825525, 0.611353711790393, 0.8242264647794602, 0.8628532046864231)\n"
     ]
    }
   ],
   "source": [
    "print(\"Results of TF-IDF - 5k\")\n",
    "for key, value in r_tfidf_5k.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"Results of TF-IDF - 15k\")\n",
    "for key, value in r_tfidf_15k.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
