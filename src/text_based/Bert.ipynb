{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 16:31:01.830367: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "#from official.nlp import optimization\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(path):\n",
    "  with open(path) as f:\n",
    "    json_data = json.load(f)\n",
    "  print(len(json_data))\n",
    "  plain_sql = [item['sql'] for item in json_data]\n",
    "  plain_sql = [sql.lower() for sql in plain_sql]\n",
    "\n",
    "  # split data into tokens\n",
    "\n",
    "  pattern = r'[\\s()\\-,:;]'\n",
    "  string_literal_pattern = r\"'([^']*)'\"\n",
    "  placeholder = \"<string>\"\n",
    "  \n",
    "  # replace content inside single quotes by <string>\n",
    "  plain_sql_ph = [re.sub(string_literal_pattern, placeholder, sql) for sql in   plain_sql]\n",
    "  \n",
    "  # split the statements with placeholder\n",
    "  tokenized_sql = [re.split(pattern, sql) for sql in plain_sql_ph]\n",
    "  \n",
    "  # remove empty tokens\n",
    "  tokenized_sql = [token for token in tokenized_sql if token]\n",
    "  \n",
    "  # replace numbers by placeholder\n",
    "  for sql in tokenized_sql:\n",
    "      for i, token in enumerate(sql):\n",
    "          # if re.match(r'^[\\'\\\"].*[\\'\\\"]$', token):  # Check if token is a   string literal\n",
    "          #     sql[i] = '<string>'\n",
    "          if re.match(r'^[0-9]+(\\.[0-9]+)?$', token):  # Check if token is a  number\n",
    "              sql[i] = '<number>'\n",
    "  \n",
    "  # remove empty tokens\n",
    "  for i, sql in enumerate(tokenized_sql):\n",
    "      tokenized_sql[i] = [token for token in tokenized_sql[i] if token]\n",
    "\n",
    "    # build the vocab\n",
    "  vocab_set = set()\n",
    "  for sql in tokenized_sql:\n",
    "      vocab_set.update(sql)\n",
    "\n",
    "  vocab_dict = {word: idx for idx, word in enumerate(vocab_set)}\n",
    "\n",
    "  # get the runtimes\n",
    "  runtime = [item['runtime_ms'] for item in json_data]\n",
    "  runtime = np.array(runtime)\n",
    "\n",
    "  # classify the runtimes, label 0 for runtime <=3000ms, 1 for runtime >3000ms\n",
    "  label = np.where(runtime > 3000, 1, 0)\n",
    "\n",
    "  return vocab_set, plain_sql_ph, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10687\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "vocab_set_15k, plain_sql_ph_15k, label_15k = tokenize(\"/Users/zhangchilu/Desktop/CSE291Cloud/291-Query-Classification/datasets/plain_text/plain_statement.json\")\n",
    "vocab_set_5k, plain_sql_ph_5k, label_5k = tokenize(\"/Users/zhangchilu/Desktop/CSE291Cloud/291-Query-Classification/datasets/plain_text/plain_statement_5000.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10687, 77)\n",
      "(5000, 77)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=vocab_set_15k)\n",
    "cv_mat_15k = vectorizer.fit_transform(plain_sql_ph_15k)\n",
    "vectorizer = CountVectorizer(vocabulary=vocab_set_5k)\n",
    "cv_mat_5k = vectorizer.fit_transform(plain_sql_ph_5k)\n",
    "print(cv_mat_15k.shape)\n",
    "print(cv_mat_5k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10687, 16162)\n",
      "(5000, 8146)\n",
      "<built-in method max of numpy.ndarray object at 0x7fcc2d866e10>\n",
      "<built-in method max of numpy.ndarray object at 0x7fcc2d866e10>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_mat_15k = tfidf_vectorizer.fit_transform(plain_sql_ph_15k)\n",
    "tfidf_mat_15k = tfidf_mat_15k.toarray()\n",
    "\n",
    "tfidf_mat_5k = tfidf_vectorizer.fit_transform(plain_sql_ph_5k)\n",
    "tfidf_mat_5k = tfidf_mat_5k.toarray()\n",
    "print(tfidf_mat_15k.shape)\n",
    "print(tfidf_mat_5k.shape)\n",
    "print(np.array([len(ql) for ql in plain_sql_ph_15k]).max)\n",
    "print(np.array([len(ql) for ql in plain_sql_ph_5k]).max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextClassificationDataset():\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        return {'input_ids': encoding['input_ids'].flatten(), 'attention_mask': encoding['attention_mask'].flatten(), 'label': torch.tensor(label)}\n",
    "    \n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.bert.requires_grad_(True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        #self.xgb = xgb.XGBClassifier()\n",
    "        #self.rf = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "        \n",
    "        self.prev_input_ids = []\n",
    "        self.prev_labels = []\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, key):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.fc(x)\n",
    "        if (key == \"LR\"):\n",
    "            self.loss = nn.CrossEntropyLoss()\n",
    "        elif (key == \"SVM\"):\n",
    "            self.loss = nn.MultiMarginLoss()\n",
    "        #logits = torch.from_numpy(self.xgb.predict_proba(x.detach().numpy()))\n",
    "        return logits\n",
    "    \n",
    "    def trainxgb(self, input_ids, labels, attention_mask):\n",
    "        if len(self.prev_input_ids) == 0 :\n",
    "            self.prev_input_ids = input_ids\n",
    "            self.prev_labels = labels\n",
    "        outputs = self.bert(input_ids=self.prev_input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        self.xgb.fit(x.detach().numpy(), self.prev_labels)\n",
    "        self.prev_input_ids = input_ids\n",
    "        self.prev_labels = labels\n",
    "\n",
    "def train(model, data_loader, optimizer, scheduler,device):\n",
    "    model.train()\n",
    "    i = 0\n",
    "    for batch in data_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        #model.trainxgb(input_ids,labels, attention_mask)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        #loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss = model.loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if (i % 10 == 0):\n",
    "            print(i)\n",
    "        i+=1\n",
    "        \n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    return (accuracy_score(actual_labels, predictions), precision_score(actual_labels, predictions, pos_label = 1), recall_score(actual_labels, predictions, pos_label = 1), precision_score(actual_labels, predictions, pos_label = 0), recall_score(actual_labels, predictions, pos_label = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = 'bert-base-uncased'\n",
    "num_classes = 2\n",
    "max_length = 128\n",
    "batch_size = 32\n",
    "num_epochs = 2\n",
    "learning_rate = 2e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(cv_mat, tfidf_mat, bert_mat, label):\n",
    "\n",
    "  X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(bert_mat, label, test_size=0.2, random_state=seed)\n",
    "  result_bert = dict()\n",
    "  \n",
    "  models = {\"LR\": LogisticRegression(max_iter=1000), \"SVM\": SVC()}\n",
    "\n",
    "  result_cv = dict()\n",
    "  result_tfidf = dict()\n",
    "\n",
    "  for key in models:\n",
    "    model = models[key]\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "    train_dataset = TextClassificationDataset(X_train_bert, y_train_bert, tokenizer, max_length)\n",
    "    val_dataset = TextClassificationDataset(X_test_bert, y_test_bert, tokenizer, max_length)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BERTClassifier(bert_model_name, num_classes, key).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_dataloader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    for epoch in range(num_epochs):\n",
    "      print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "      train(model, train_dataloader, optimizer, scheduler, device)\n",
    "      result_bert[key] = evaluate(model, val_dataloader, device)\n",
    "      print(f\"Validation Accuracy:\" ,result_bert[key])\n",
    "      print(\"Completed training of model {key} for Bert\")\n",
    "\n",
    "  \n",
    "  return result_cv, result_tfidf, result_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangchilu/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "Validation Accuracy: (0.738, 0.5795454545454546, 0.7680722891566265, 0.8625, 0.7230538922155688)\n",
      "(0.738, 0.5795454545454546, 0.7680722891566265, 0.8625, 0.7230538922155688)\n",
      "Epoch 2/2\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "Validation Accuracy: (0.745, 0.5955334987593052, 0.7228915662650602, 0.8458961474036851, 0.7559880239520959)\n",
      "(0.745, 0.5955334987593052, 0.7228915662650602, 0.8458961474036851, 0.7559880239520959)\n",
      "Completed training of model {key} for BoW\n",
      "Completed training of model {key} for TF-IDF\n",
      "Completed training of model {key} for BoW\n",
      "Completed training of model {key} for TF-IDF\n",
      "Completed training of model {key} for BoW\n",
      "Completed training of model {key} for TF-IDF\n",
      "268\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangchilu/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "Validation Accuracy: (0.7422825070159027, 0.6122112211221122, 0.5400291120815138, 0.793733681462141, 0.8380427291523087)\n",
      "(0.7422825070159027, 0.6122112211221122, 0.5400291120815138, 0.793733681462141, 0.8380427291523087)\n",
      "Epoch 2/2\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "Validation Accuracy: (0.7347988774555659, 0.5887573964497042, 0.5793304221251819, 0.8023255813953488, 0.808407994486561)\n",
      "(0.7347988774555659, 0.5887573964497042, 0.5793304221251819, 0.8023255813953488, 0.808407994486561)\n",
      "Completed training of model {key} for BoW\n",
      "Completed training of model {key} for TF-IDF\n",
      "Completed training of model {key} for BoW\n",
      "Completed training of model {key} for TF-IDF\n",
      "Completed training of model {key} for BoW\n",
      "Completed training of model {key} for TF-IDF\n"
     ]
    }
   ],
   "source": [
    "r_cv_5k, r_tfidf_5k, r_bert_5k = run_exp(cv_mat_5k, tfidf_mat_5k, plain_sql_ph_5k, label_5k)\n",
    "r_cv_15k, r_tfidf_15k, r_bert_15k = run_exp(cv_mat_15k, tfidf_mat_15k, plain_sql_ph_15k, label_15k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of BoW - 5k\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'r_cv_5k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults of BoW - 5k\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mr_cv_5k\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults of BoW - 15k\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'r_cv_5k' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Results of BoW - 5k\")\n",
    "for key, value in r_cv_5k.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"Results of BoW - 15k\")\n",
    "for key, value in r_cv_15k.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results of TF-IDF - 5k\")\n",
    "for key, value in r_tfidf_5k.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print(\"Results of TF-IDF - 15k\")\n",
    "for key, value in r_tfidf_15k.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PyTorch Embedding\n",
    "\n",
    "# define embedding layer\n",
    "\n",
    "# vocab_size = len(vocab_dict)\n",
    "# embedding_dim = 10\n",
    "# embedding = nn.Embedding(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tokens to indices for each sample\n",
    "# indices = [torch.LongTensor([vocab_dict[token] for token in sql]) for sql in tokenized_sql]\n",
    "\n",
    "# X_torch = []\n",
    "\n",
    "# for index in indices:\n",
    "#     emb = embedding(index)\n",
    "#     sum = torch.sum(emb, dim=0)\n",
    "#     X_torch.append(sum.tolist())\n",
    "\n",
    "# X_torch = np.array(X_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
