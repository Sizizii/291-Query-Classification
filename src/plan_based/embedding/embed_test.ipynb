{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_embed(node_param, max_filter, num_filter_attr, op_stats, col_id_map, col_stats, table_id_map, table_stats, type_id_map, filter_op_id_map, num_attr):\n",
    "\n",
    "  # operations\n",
    "  op_vec = np.array([0 for _ in range(len(op_stats))])\n",
    "  op_vec[op_stats[node_param['op_name']]] = 1\n",
    "\n",
    "  # attributes\n",
    "  attr_vec = np.array([0.0 for _ in range(num_attr)])\n",
    "  attr_vec[0] = node_param['est_card']\n",
    "  attr_vec[1] = node_param['est_width']\n",
    "  if 'table' in node_param:\n",
    "    table_name = table_id_map[str(node_param['table'])]\n",
    "    table_stat = table_stats[table_name]\n",
    "    attr_vec[2] = table_stat['reltuples']\n",
    "    attr_vec[3] = table_stat['relpages']\n",
    "\n",
    "\n",
    "  # predicates\n",
    "  # col filter_op literal\n",
    "  filter_vec = np.array([0.0 for _ in range(max_filter * num_filter_attr)])\n",
    "  if 'filter_columns' in node_param:\n",
    "    filter_idx = 0\n",
    "    num_data_type = len(type_id_map)\n",
    "    # 5 col statistics are used\n",
    "    num_col_fileds = num_data_type + 5\n",
    "    num_filter_op = len(filter_op_id_map)\n",
    "\n",
    "    filters = [node_param['filter_columns']]\n",
    "    if filters[0]['operator'] == \"AND\":\n",
    "      filters = node_param['filter_columns']['children']\n",
    "\n",
    "    for fil in filters:\n",
    "      print(fil)\n",
    "      # vector includes:\n",
    "      # column related:\n",
    "      #   col data type, num_unique, nan_ratio, attr correlation with row number, avg width, table_size\n",
    "      # filter_op\n",
    "      # literal_related: unique values of literal\n",
    "      fil_start = filter_idx * num_filter_attr\n",
    "      col_stat = col_stats[col_id_map[str(fil['column'])]]\n",
    "      col_data_type = col_stat[\"datatype\"]\n",
    "      filter_vec[fil_start + type_id_map[col_data_type]] = 1\n",
    "      filter_vec[fil_start + num_data_type] = col_stat[\"num_unique\"]\n",
    "      filter_vec[fil_start + num_data_type + 1] = col_stat[\"nan_ratio\"]\n",
    "      filter_vec[fil_start + num_data_type + 2] = col_stat[\"corr_with_row\"]\n",
    "      filter_vec[fil_start + num_data_type + 3] = col_stat[\"avg_width\"]\n",
    "      filter_vec[fil_start + num_data_type + 4] = col_stat[\"table_size\"]\n",
    "\n",
    "      filter_op_idx = filter_op_id_map[fil['operator']]\n",
    "      filter_vec[fil_start + num_col_fileds + filter_op_idx] = 1\n",
    "\n",
    "      literal = fil['literal']\n",
    "      filter_vec[fil_start + num_col_fileds + num_filter_op] = 1\n",
    "      if isinstance(literal, str):\n",
    "        l_col = literal.strip()\n",
    "        if re.match(r'^\\w+\\.\\w+$', l_col):\n",
    "          filter_vec[fil_start + num_col_fileds + num_filter_op] = col_stats[l_col][\"num_unique\"]\n",
    "\n",
    "      filter_idx += 1\n",
    "\n",
    "  # outputs\n",
    "  # output aggregation type inclue COUNT, AVG, SUM, and NONE\n",
    "  num_col = len(col_id_map)\n",
    "  output_vec = np.array([0 for _ in range(num_col * 3 + 1)])\n",
    "  output_avg_start = 1\n",
    "  output_sum_start = num_col + 1\n",
    "  output_none_start = 2*num_col + 1\n",
    "  if 'output_columns' in node_param:\n",
    "    for output in node_param['output_columns']:\n",
    "      agg_op = output['aggregation']\n",
    "\n",
    "      if agg_op == 'COUNT':\n",
    "        output_vec[0] = 1\n",
    "      elif agg_op == 'AVG':\n",
    "        for output_col in output['columns']:\n",
    "          output_vec[output_avg_start + output_col] = 1\n",
    "      elif agg_op == \"SUM\":\n",
    "        for output_col in output['columns']:\n",
    "          output_vec[output_sum_start + output_col] = 1\n",
    "      else:\n",
    "        for output_col in output['columns']:\n",
    "          output_vec[output_none_start + output_col] = 1\n",
    "\n",
    "  return op_vec, attr_vec, filter_vec, output_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"../../../datasets/stats/stat_complete.json\", \"r\") as f:\n",
    "  stats = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stats)\n",
    "node_param, max_filter, num_filter_attr, op_stats, col_id_map, col_stats, table_id_map, table_stats, type_id_map, filter_op_id_map, num_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_id_map, col_stats, table_id_map, table_stats, type_id_map, op_stats, filter_op_id_map = stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_5k = \"../../../datasets/plans/parsed/workload_5k_s1_c8220.json\"\n",
    "with open(parsed_5k, \"r\") as f:\n",
    "    parsed_5k_plan = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_param = parsed_5k_plan['parsed_plans'][0]['children'][0]['plan_parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for plan in parsed_5k_plan['parsed_plans']:\n",
    "  level = 0\n",
    "\n",
    "  queue = deque([plan])\n",
    "\n",
    "  while queue:\n",
    "    level_count = len(queue)\n",
    "    while level_count > 0:\n",
    "      level_count -= 1\n",
    "      node = queue.popleft()\n",
    "      param = node['plan_parameters']\n",
    "\n",
    "      if 'filter_columns' in param:\n",
    "        # print(param['filter_columns'])\n",
    "        op = param['filter_columns']['operator']\n",
    "        if op == \"AND\":\n",
    "          node_param = param\n",
    "          break\n",
    "\n",
    "      for children in node['children']:\n",
    "        queue.append(children)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'table': 6,\n",
       " 'op_name': 'Seq Scan',\n",
       " 'est_startup_cost': 0.0,\n",
       " 'est_cost': 48595.0,\n",
       " 'est_card': 1192098.0,\n",
       " 'est_width': 8.0,\n",
       " 'act_startup_cost': 0.012,\n",
       " 'act_time': 399.803,\n",
       " 'act_card': 1198171.0,\n",
       " 'filter_columns': {'column': None,\n",
       "  'operator': 'AND',\n",
       "  'literal': None,\n",
       "  'literal_feature': None,\n",
       "  'children': [{'column': 42,\n",
       "    'operator': '!=',\n",
       "    'literal': 'Clerk#000000918',\n",
       "    'literal_feature': 0,\n",
       "    'children': []},\n",
       "   {'column': 41,\n",
       "    'operator': '!=',\n",
       "    'literal': '5-LOW',\n",
       "    'literal_feature': 0,\n",
       "    'children': []}]},\n",
       " 'output_columns': [{'aggregation': 'None', 'columns': [36]},\n",
       "  {'aggregation': 'None', 'columns': [37]},\n",
       "  {'aggregation': 'None', 'columns': [38]},\n",
       "  {'aggregation': 'None', 'columns': [39]},\n",
       "  {'aggregation': 'None', 'columns': [40]},\n",
       "  {'aggregation': 'None', 'columns': [41]},\n",
       "  {'aggregation': 'None', 'columns': [42]},\n",
       "  {'aggregation': 'None', 'columns': [43]},\n",
       "  {'aggregation': 'None', 'columns': [44]}],\n",
       " 'act_children_card': 1,\n",
       " 'est_children_card': 1,\n",
       " 'workers_planned': 2}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'column': 42, 'operator': '!=', 'literal': 'Clerk#000000918', 'literal_feature': 0, 'children': []}\n",
      "{'column': 41, 'operator': '!=', 'literal': '5-LOW', 'literal_feature': 0, 'children': []}\n"
     ]
    }
   ],
   "source": [
    "op_vec, attr_vec, filter_vec, output_vec = node_embed(node_param, 5, 14, op_stats, col_id_map, col_stats, table_id_map, table_stats, type_id_map, filter_op_id_map, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 1.00000e+03 0.00000e+00\n",
      " 8.56727e-03 1.60000e+01 1.50000e+06 0.00000e+00 0.00000e+00 0.00000e+00\n",
      " 1.00000e+00 1.00000e+00]\n",
      "[0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00 5.00000e+00 0.00000e+00\n",
      " 1.99784e-01 1.60000e+01 1.50000e+06 0.00000e+00 0.00000e+00 0.00000e+00\n",
      " 1.00000e+00 1.00000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(filter_vec[:14])\n",
    "print(filter_vec[14:28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n",
      "0\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(len(output_vec))\n",
    "print(output_vec[0])\n",
    "print(output_vec[1:62])\n",
    "print(output_vec[62:123])\n",
    "print(output_vec[123:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_embed(root_node, max_filter, num_filter_attr, op_stats, col_id_map, col_stats, table_id_map, table_stats, type_id_map, filter_op_id_map, num_attr):\n",
    "\n",
    "  node_queue = deque([root_node])\n",
    "\n",
    "  plan_op, plan_attr, plan_filter, plan_output, plan_mapping = [], [], [], [], []\n",
    "\n",
    "  while node_queue:\n",
    "    level_count = len(node_queue)\n",
    "\n",
    "    plan_op.append([])\n",
    "    plan_attr.append([])\n",
    "    plan_filter.append([])\n",
    "    plan_output.append([])\n",
    "    plan_mapping.append([])\n",
    "\n",
    "    children_sum = 0\n",
    "\n",
    "    while level_count > 0:\n",
    "      level_count -= 1\n",
    "      node = node_queue.popleft()\n",
    "      node_param = node.plan_parameters\n",
    "      op_vec, attr_vec, filter_vec, output_vec = node_embed(node_param, max_filter, num_filter_attr, op_stats, col_id_map, col_stats, table_id_map, table_stats, type_id_map, filter_op_id_map, num_attr)\n",
    "\n",
    "      plan_op[-1].append(op_vec)\n",
    "      plan_attr[-1].append(attr_vec)\n",
    "      plan_filter[-1].append(filter_vec)\n",
    "      plan_output[-1].append(output_vec)\n",
    "\n",
    "      if len(node.children) == 2:\n",
    "        plan_mapping[-1].append([child.idx + children_sum for child in node.children])\n",
    "      elif len(node.children) == 1:\n",
    "        plan_mapping[-1].append([node.children[0].idx + children_sum, 0])\n",
    "      else:\n",
    "        plan_mapping[-1].append([0,0])\n",
    "\n",
    "      children_sum += len(node.children)\n",
    "\n",
    "      for child in node.children:\n",
    "        node_queue.append(child)\n",
    "\n",
    "  return plan_op, plan_attr, plan_filter, plan_output, plan_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict\n",
    "\n",
    "class SQLNode():\n",
    "    def __init__(self, plain_content:List, plan_parameters:Dict,  plan_runtime: float) -> None:\n",
    "        self.plain_content = plain_content\n",
    "        self.plan_parameters = plan_parameters\n",
    "        self.children : Optional[List['SQLNode']] = []\n",
    "        self.plan_runtime = plan_runtime\n",
    "        self.idx = 1\n",
    "        # self.join_conds = join_conds\n",
    "\n",
    "\n",
    "class SQLTree():\n",
    "    def __init__(self, parsed_sql:Dict) -> None:\n",
    "        plain_content = parsed_sql[\"plain_content\"]\n",
    "        plan_parameters = parsed_sql[\"plan_parameters\"]\n",
    "        children = parsed_sql[\"children\"]\n",
    "        plan_runtime = parsed_sql[\"plan_runtime\"]\n",
    "        # join_conds = parsed_sql[\"join_conds\"]\n",
    "        self.root : Optional[SQLNode] = SQLNode(plain_content, plan_parameters, plan_runtime)\n",
    "        self._insert_children_sql(self.root, children)\n",
    "        \n",
    "    def _insert_children_sql(self, node: SQLNode, children):\n",
    "        for parsed_sql in children:\n",
    "            plain_content = parsed_sql[\"plain_content\"]\n",
    "            plan_parameters = parsed_sql[\"plan_parameters\"]\n",
    "            children = parsed_sql[\"children\"]\n",
    "            plan_runtime = parsed_sql[\"plan_runtime\"]\n",
    "            # join_conds = parsed_sql[\"join_conds\"]\n",
    "            new_node = SQLNode(plain_content, plan_parameters, plan_runtime)\n",
    "            new_node.idx = len(node.children) + 1\n",
    "            node.children.append(new_node)\n",
    "            self._insert_children_sql(new_node, children)\n",
    "        \n",
    "        return\n",
    "\n",
    "class workload_dataloader():\n",
    "    def __init__(self, filename) -> None:\n",
    "        with open(filename, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        parsed_sqls = data[\"parsed_plans\"]\n",
    "        self.sql_forest = []\n",
    "        for parsed_sql in parsed_sqls:\n",
    "            tree = SQLTree(parsed_sql)\n",
    "            self.sql_forest.append(tree)\n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.sql_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../../../datasets/plans/parsed/workload_5k_s1_c8220.json\"\n",
    "workload = workload_dataloader(filename)\n",
    "tree = workload.get_data()[1293]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_node = tree.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'column': 37, 'operator': '=', 'literal': 'customer.c_custkey  ', 'literal_feature': 0, 'children': []}\n",
      "{'column': 40, 'operator': '!=', 'literal': '1992-07-27', 'literal_feature': 0, 'children': []}\n",
      "{'column': 42, 'operator': '=', 'literal': 'Clerk#000000682', 'literal_feature': 0, 'children': []}\n",
      "{'column': 28, 'operator': '>=', 'literal': 120925.0, 'literal_feature': 0, 'children': []}\n",
      "{'column': 31, 'operator': '<=', 'literal': 0.0, 'literal_feature': 0, 'children': []}\n"
     ]
    }
   ],
   "source": [
    "plan_op, plan_attr, plan_filter, plan_output, plan_mapping = plan_embed(root_node, 5, 14, op_stats, col_id_map, col_stats, table_id_map, table_stats, type_id_map, filter_op_id_map, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[[1, 0]]\n",
      "[[1, 2]]\n",
      "[[1, 0], [2, 0]]\n",
      "[[0, 0], [1, 0]]\n",
      "[[0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(len(plan_mapping))\n",
    "for maps in plan_mapping:\n",
    "  print(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan_output[2][0][123:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])]\n",
      "[array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])]\n",
      "[array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]), array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "[array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "for op in plan_op:\n",
    "  print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'column': 37, 'operator': '=', 'literal': 'customer.c_custkey  ', 'literal_feature': 0, 'children': []}\n",
      "{'column': 43, 'operator': '=', 'literal': 0.0, 'literal_feature': 0, 'children': []}\n",
      "6\n",
      "[[1, 0]]\n",
      "[[1, 0]]\n",
      "[[1, 0]]\n",
      "[[1, 2]]\n",
      "[[0, 0], [1, 0]]\n",
      "[[0, 0]]\n"
     ]
    }
   ],
   "source": [
    "tree = workload.get_data()[1294]\n",
    "root_node = tree.root\n",
    "plan_op, plan_attr, plan_filter, plan_output, plan_mapping = plan_embed(root_node, 5, 14, op_stats, col_id_map, col_stats, table_id_map, table_stats, type_id_map, filter_op_id_map, 4)\n",
    "print(len(plan_mapping))\n",
    "for maps in plan_mapping:\n",
    "  print(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_embed(plan_trees, max_filter, num_filter_attr, op_stats, col_id_map, col_stats, table_id_map, table_stats, type_id_map, filter_op_id_map, num_attr):\n",
    "\n",
    "  target_runtime = []\n",
    "  batch_op, batch_attr, batch_filter, batch_output, batch_mapping = [], [], [], [], []\n",
    "\n",
    "  for plan_tree in plan_trees:\n",
    "    plan_root = plan_tree.root\n",
    "    target_runtime.append(plan_root.plan_runtime)\n",
    "\n",
    "    plan_op, plan_attr, plan_filter, plan_output, plan_mapping = plan_embed(plan_root, max_filter, num_filter_attr, op_stats, col_id_map, col_stats, table_id_map, table_stats, type_id_map, filter_op_id_map, num_attr)\n",
    "\n",
    "    batch_op = _batch_merge_vec(batch_op, plan_op)\n",
    "    batch_attr = _batch_merge_vec(batch_attr, plan_attr)\n",
    "    batch_filter = _batch_merge_vec(batch_filter, plan_filter)\n",
    "    batch_output = _batch_merge_vec(batch_output, plan_output)\n",
    "\n",
    "    batch_mapping = _batch_merge_mapping(batch_mapping, plan_mapping)\n",
    "\n",
    "\n",
    "  max_num_node_per_level = 0\n",
    "  for op in batch_op:\n",
    "    max_num_node_per_level = max(max_num_node_per_level, len(op))\n",
    "\n",
    "  print(max_num_node_per_level)\n",
    "  \n",
    "  batch_op_pad = pad_to_max_len(batch_op, max_num_node_per_level)\n",
    "  batch_attr_pad = pad_to_max_len(batch_attr, max_num_node_per_level)\n",
    "  batch_filter_pad = pad_to_max_len(batch_filter, max_num_node_per_level)\n",
    "  batch_output_pad = pad_to_max_len(batch_output, max_num_node_per_level)\n",
    "  batch_mapping_pad = pad_to_max_len(batch_mapping, max_num_node_per_level)\n",
    "\n",
    "  return target_runtime, batch_op_pad, batch_attr_pad, batch_filter_pad, batch_output_pad, batch_mapping_pad, batch_op, batch_attr, batch_filter, batch_output, batch_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _batch_merge_vec(batch_vec, plan_vec):\n",
    "  # if plan has larger number of levels, create a new level list and append\n",
    "  for level, level_vec in enumerate(plan_vec):\n",
    "    if level >= len(batch_vec):\n",
    "      batch_vec.append([])\n",
    "    batch_vec[level] += level_vec\n",
    "\n",
    "  return batch_vec\n",
    "\n",
    "def _batch_merge_mapping(batch_map, plan_map):\n",
    "  for level, level_map in enumerate(plan_map):\n",
    "    if level >= len(batch_map):\n",
    "      batch_map.append([])\n",
    "    \n",
    "    # to find if there are preceding parent nodes on the same level\n",
    "    # if found, should increments its children node mapping (on next level)\n",
    "    if level < len(batch_map) - 1:\n",
    "      batch_level_base = len(batch_map[level + 1])\n",
    "      for i in range(len(level_map)):\n",
    "        if level_map[i][0] > 0:\n",
    "          level_map[i][0] += batch_level_base\n",
    "        if level_map[i][1] > 0:\n",
    "          level_map[i][1] += batch_level_base\n",
    "      \n",
    "    batch_map[level] += level_map\n",
    "\n",
    "  return batch_map\n",
    "\n",
    "def pad_to_max_len(batch_vec, max_num):\n",
    "  return np.array([np.pad(vec, ((0, max_num - len(vec)), (0, 0)), 'constant') for vec in batch_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = workload.get_data()[1293:1295]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'column': 37, 'operator': '=', 'literal': 'customer.c_custkey  ', 'literal_feature': 0, 'children': []}\n",
      "{'column': 40, 'operator': '!=', 'literal': '1992-07-27', 'literal_feature': 0, 'children': []}\n",
      "{'column': 42, 'operator': '=', 'literal': 'Clerk#000000682', 'literal_feature': 0, 'children': []}\n",
      "{'column': 28, 'operator': '>=', 'literal': 120925.0, 'literal_feature': 0, 'children': []}\n",
      "{'column': 31, 'operator': '<=', 'literal': 0.0, 'literal_feature': 0, 'children': []}\n",
      "{'column': 37, 'operator': '=', 'literal': 'customer.c_custkey  ', 'literal_feature': 0, 'children': []}\n",
      "{'column': 43, 'operator': '=', 'literal': 0.0, 'literal_feature': 0, 'children': []}\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# plan_op, plan_attr, plan_filter, plan_output, plan_mapping = plan_embed(root_node, 5, 14, op_stats, col_id_map, col_stats, table_id_map, table_stats, type_id_map, filter_op_id_map, 4)\n",
    "target_runtime, batch_op_pad, batch_attr_pad, batch_filter_pad, batch_output_pad, batch_mapping_pad, batch_op, batch_attr, batch_filter, batch_output, batch_mapping = batch_embed(trees, 5, 14, op_stats, col_id_map, col_stats, table_id_map, table_stats, type_id_map, filter_op_id_map, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6\n",
      "With pad 3 [[ 1.  8.  0.  0.]\n",
      " [ 1. 72.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "Without pad 2 [array([1., 8., 0., 0.]), array([ 1., 72.,  0.,  0.])]\n",
      "With pad 3 [[11.  0.  0.  0.]\n",
      " [ 2. 72.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "Without pad 2 [array([11.,  0.,  0.,  0.]), array([ 2., 72.,  0.,  0.])]\n",
      "With pad 3 [[1.448e+03 4.000e+00 0.000e+00 0.000e+00]\n",
      " [1.118e+03 4.000e+00 0.000e+00 0.000e+00]\n",
      " [1.000e+00 7.200e+01 0.000e+00 0.000e+00]]\n",
      "Without pad 3 [array([1448.,    4.,    0.,    0.]), array([1118.,    4.,    0.,    0.]), array([ 1., 72.,  0.,  0.])]\n",
      "With pad 3 [[6.0300e+02 4.0000e+00 1.5000e+06 2.6095e+04]\n",
      " [1.1180e+03 4.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [6.2500e+05 1.6000e+01 0.0000e+00 0.0000e+00]]\n",
      "Without pad 3 [array([6.0300e+02, 4.0000e+00, 1.5000e+06, 2.6095e+04]), array([1118.,    4.,    0.,    0.]), array([6.25e+05, 1.60e+01, 0.00e+00, 0.00e+00])]\n",
      "With pad 3 [[4.6600e+02 4.0000e+00 1.5000e+05 3.5850e+03]\n",
      " [6.2500e+05 1.6000e+01 1.5000e+06 2.6095e+04]\n",
      " [1.5000e+05 4.0000e+00 0.0000e+00 0.0000e+00]]\n",
      "Without pad 3 [array([4.660e+02, 4.000e+00, 1.500e+05, 3.585e+03]), array([6.2500e+05, 1.6000e+01, 1.5000e+06, 2.6095e+04]), array([1.5e+05, 4.0e+00, 0.0e+00, 0.0e+00])]\n",
      "With pad 3 [[1.500e+05 4.000e+00 1.500e+05 3.585e+03]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00]]\n",
      "Without pad 1 [array([1.500e+05, 4.000e+00, 1.500e+05, 3.585e+03])]\n"
     ]
    }
   ],
   "source": [
    "print(len(batch_attr_pad), len(batch_attr))\n",
    "\n",
    "for i in range(len(batch_attr_pad)):\n",
    "  level_attr_pad = batch_attr_pad[i]\n",
    "  level_attr = batch_attr[i]\n",
    "  print(\"With pad\", len(level_attr_pad), level_attr_pad)\n",
    "  print(\"Without pad\", len(level_attr), level_attr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6\n",
      "With pad [[1 0]\n",
      " [2 0]\n",
      " [0 0]]\n",
      "Without pad [[1, 0], [2, 0]]\n",
      "With pad [[1 2]\n",
      " [3 0]\n",
      " [0 0]]\n",
      "Without pad [[1, 2], [3, 0]]\n",
      "With pad [[1 0]\n",
      " [2 0]\n",
      " [3 0]]\n",
      "Without pad [[1, 0], [2, 0], [3, 0]]\n",
      "With pad [[0 0]\n",
      " [1 0]\n",
      " [2 3]]\n",
      "Without pad [[0, 0], [1, 0], [2, 3]]\n",
      "With pad [[0 0]\n",
      " [0 0]\n",
      " [1 0]]\n",
      "Without pad [[0, 0], [0, 0], [1, 0]]\n",
      "With pad [[0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "Without pad [[0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(len(batch_mapping_pad), len(batch_mapping))\n",
    "\n",
    "for i in range(len(batch_mapping_pad)):\n",
    "  level_map_pad = batch_mapping_pad[i]\n",
    "  level_map = batch_mapping[i]\n",
    "  print(\"With pad\", level_map_pad)\n",
    "  print(\"Without pad\", level_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
